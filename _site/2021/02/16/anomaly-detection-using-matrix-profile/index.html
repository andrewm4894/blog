<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Anomaly Detection using the Matrix Profile | Andrew M Blog</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Anomaly Detection using the Matrix Profile" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Personal blog about Data Science and Machine Learning, migrated from WordPress." />
<meta property="og:description" content="Personal blog about Data Science and Machine Learning, migrated from WordPress." />
<link rel="canonical" href="http://localhost:4000/2021/02/16/anomaly-detection-using-matrix-profile/" />
<meta property="og:url" content="http://localhost:4000/2021/02/16/anomaly-detection-using-matrix-profile/" />
<meta property="og:site_name" content="Andrew M Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-16T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Anomaly Detection using the Matrix Profile" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-02-16T00:00:00+00:00","datePublished":"2021-02-16T00:00:00+00:00","description":"Personal blog about Data Science and Machine Learning, migrated from WordPress.","headline":"Anomaly Detection using the Matrix Profile","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2021/02/16/anomaly-detection-using-matrix-profile/"},"url":"http://localhost:4000/2021/02/16/anomaly-detection-using-matrix-profile/"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="http://localhost:4000/">Andrew M Blog</a></h1>

        

        <p>Personal blog about Data Science and Machine Learning, migrated from WordPress.
</p>

        

        

        
      </header>
      <section>

      

<h1>Anomaly Detection using the Matrix Profile</h1>

<p class="post-meta">
  <time datetime="2021-02-16T00:00:00+00:00">Feb 16, 2021
  </time><span class="categories">
    • Categories: 
    
    <span class="category">anomaly-detection</span>
    , 
    
    <span class="category">machine-learning</span>
    , 
    
    <span class="category">time-series</span>
    
    
  </span></p>



<p><img src="/assets/images/2021-02-16-anomaly-detection-using-matrix-profile/distance_matrix.gif" alt="" /></p>

<p>I like an excuse to play with fancy things, so when i first learned about the <a href="https://matrixprofile.org/#:~:text=The%20matrix%20profile%20is%20a,scalable%20and%20largely%20parameter%2Dfree.">Matrix Profile</a> for time series analysis, particularly around anomaly detection, i was intrigued. When i learned there was a nice python package (<a href="https://stumpy.readthedocs.io/en/latest/index.html">STUMPY</a>) i could just pip install i was outright excited, as one thing i like more than something fancy, is something fancy where someone has already done most of the work for me :)</p>

<p><strong>Note</strong>: <a href="https://colab.research.google.com/drive/1Pkzk0PVWThNePdjVOTZrxERCXPB2b_Sw?usp=sharing">here is a colab notebook</a> you can just run and play with for yourself.</p>

<p>My use case is time series anomaly detection on monitoring data from <a href="http://github.com/netdata/netdata">Netdata</a> (where i work). <a href="http://london.my-netdata.io/">Here</a> is an example demo dashboard i will use for illustration. Basically what i want to do is pull an hour of data from my server, compute the matrix profile for each metric in that data (about 1500 metrics) and then use the matrix profile for each metric to in some way rank metrics and surface the most anomalous looking via some sort of scoring and ranking.</p>

<p><strong>tl; dr</strong>: the Matrix Profile computes subsequence distances within your time series and so subsequences with a larger distance to any other observed subsequences in your time series are more unique in some sense of being the most distant/different and so, possibly, anomalous. There are lots of other ways to use the Matrix Profile and the <a href="https://stumpy.readthedocs.io/en/latest/Tutorial_The_Matrix_Profile.html">STUMPY tutorials</a> do a great job explaining them. Or if you are more of a YouTube video type of person (as i am) check <a href="https://youtu.be/WvaBPSeA_JA">this one</a> out.</p>

<p>So i need to compute all the matrix profiles, and then also score and rank them in some way to find what are the most interesting ones in terms of my anomaly detection use case.</p>

<p>First let’s define some inputs and pull some data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># install required libraries as needed
#!pip install netdata-pandas scipy==1.5.4 stumpy==1.8.0 plotly am4894plots
</span>
<span class="c1"># imports
</span><span class="k">from</span><span class="err"> </span><span class="n">datetime</span><span class="err"> </span><span class="k">import</span><span class="err"> </span><span class="n">timedelta</span>
<span class="k">import</span><span class="err"> </span><span class="n">numpy</span><span class="err"> </span><span class="k">as</span><span class="err"> </span><span class="n">np</span>
<span class="k">import</span><span class="err"> </span><span class="n">pandas</span><span class="err"> </span><span class="k">as</span><span class="err"> </span><span class="n">pd</span>
<span class="k">import</span><span class="err"> </span><span class="n">time</span>
<span class="k">import</span><span class="err"> </span><span class="n">stumpy</span>
<span class="k">from</span><span class="err"> </span><span class="n">netdata_pandas</span><span class="p">.</span><span class="n">data</span><span class="err"> </span><span class="k">import</span><span class="err"> </span><span class="n">get_data</span>
<span class="k">from</span><span class="err"> </span><span class="n">am4894plots</span><span class="p">.</span><span class="n">plots</span><span class="err"> </span><span class="k">import</span><span class="err"> </span><span class="n">plot_lines</span><span class="p">,</span><span class="err"> </span><span class="n">plot_lines_grid</span>

<span class="c1"># inputs
</span><span class="n">hosts</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="p">[</span><span class="s">'london.my-netdata.io'</span><span class="p">]</span>
<span class="n">charts_regex</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="s">'.*'</span>
<span class="n">before</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="mi">0</span>
<span class="n">after</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="o">-</span><span class="mi">60</span><span class="o">*</span><span class="mi">60</span><span class="o">*</span><span class="mi">1</span>
<span class="n">smooth_n</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="mi">30</span>

<span class="c1"># get the data
</span><span class="n">df</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">get_data</span><span class="p">(</span><span class="n">hosts</span><span class="o">=</span><span class="n">hosts</span><span class="p">,</span><span class="err"> </span><span class="n">charts_regex</span><span class="o">=</span><span class="n">charts_regex</span><span class="p">,</span><span class="err"> </span><span class="n">after</span><span class="o">=</span><span class="n">after</span><span class="p">,</span><span class="err"> </span><span class="n">before</span><span class="o">=</span><span class="n">before</span><span class="p">,</span><span class="err"> </span><span class="n">index_as_datetime</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">df</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">df</span><span class="p">[[</span><span class="n">col</span><span class="err"> </span><span class="k">for</span><span class="err"> </span><span class="n">col</span><span class="err"> </span><span class="ow">in</span><span class="err"> </span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="err"> </span><span class="k">if</span><span class="err"> </span><span class="s">'uptime'</span><span class="err"> </span><span class="ow">not</span><span class="err"> </span><span class="ow">in</span><span class="err"> </span><span class="n">col</span><span class="p">]]</span>
<span class="n">df</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">df</span><span class="p">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">smooth_n</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">dropna</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">df</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>

<p>Now we are in our happy place, we have all the data we need in a nice clean pandas dataframe. About ~3500 rows (one per second) and ~1500 columns each of which is a time series metric we are monitoring.</p>

<p><img src="/assets/images/2021-02-16-anomaly-detection-using-matrix-profile/image-1024x312.png" alt="" /></p>

<p>The next step is to use STUMPY to generate the matrix profile for each metric (or a large sample of them which is what i have done in the notebook for convenience while exploring the idea). Then we also need to decide what matrix profiles are ‘good’ ones for anomaly detection. It’s not much use just returning ~1500 matrix profiles to a user - we need to try surface the most interesting ones to the top of the pile.</p>

<p>One of the main things to remember about the matrix profile is that the higher values represent sequences in the time series that are the furthest away from any other sub sequences and as such these are where the anomalies are most likely to be.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define some objects to store results in
</span><span class="n">scores</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="p">{}</span>
<span class="n">mp_dists</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="p">{}</span>

<span class="c1"># sample n_cols just to be quicker
</span><span class="n">n_cols</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="mi">500</span>

<span class="c1"># sample some metrics
</span><span class="n">df_samp</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">df</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span><span class="err"> </span><span class="n">n_cols</span><span class="p">,</span><span class="err"> </span><span class="n">replace</span><span class="o">=</span><span class="bp">False</span><span class="p">)]</span>

<span class="c1"># normalize the data prior to computing the mp so that the scores we use are all somewhat comparable across metrics on different scales
</span><span class="n">df_samp</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="p">((</span><span class="n">df_samp</span><span class="o">-</span><span class="n">df_samp</span><span class="p">.</span><span class="nb">min</span><span class="p">())</span><span class="o">/</span><span class="p">(</span><span class="n">df_samp</span><span class="p">.</span><span class="nb">max</span><span class="p">()</span><span class="o">-</span><span class="n">df_samp</span><span class="p">.</span><span class="nb">min</span><span class="p">()))</span>
<span class="n">df_samp</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">df_samp</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">how</span><span class="o">=</span><span class="s">'all'</span><span class="p">,</span><span class="err"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">df_samp</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">df_samp</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">how</span><span class="o">=</span><span class="s">'all'</span><span class="p">,</span><span class="err"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># loop over each metric
</span><span class="k">for</span><span class="err"> </span><span class="n">col</span><span class="err"> </span><span class="ow">in</span><span class="err"> </span><span class="n">df_samp</span><span class="p">.</span><span class="n">columns</span><span class="p">:</span>

<span class="err">    </span><span class="c1"># get the mp
</span><span class="err">    </span><span class="n">m</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="mi">60</span>
<span class="err">    </span><span class="n">x</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">df_samp</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="c1">#.diff().dropna()
</span><span class="err">    </span><span class="n">mp</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">stumpy</span><span class="p">.</span><span class="n">stump</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="err"> </span><span class="n">m</span><span class="o">=</span><span class="n">m</span><span class="p">,</span><span class="err"> </span><span class="n">ignore_trivial</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="err">    </span><span class="n">mp_dist</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">mp</span><span class="p">[:,</span><span class="err"> </span><span class="mi">0</span><span class="p">]</span>

<span class="err">    </span><span class="c1"># score the mp based on distance between the 99.5th percentile and 80th percentile
</span><span class="err">    </span><span class="n">df_mp_dist</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">mp_dist</span><span class="p">).</span><span class="n">rolling</span><span class="p">(</span><span class="n">smooth_n</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
<span class="err">    </span><span class="n">score</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">df_mp_dist</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.995</span><span class="p">)</span><span class="err"> </span><span class="o">-</span><span class="err"> </span><span class="n">df_mp_dist</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">quantile</span><span class="p">(</span><span class="mf">0.8</span><span class="p">)</span>
<span class="err">    </span><span class="n">scores</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">score</span>

<span class="err">    </span><span class="c1"># pad out the start of the mp_dist with nan's
</span><span class="err">    </span><span class="n">n_fill</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="nb">len</span><span class="p">(</span><span class="n">df_samp</span><span class="p">[</span><span class="n">col</span><span class="p">])</span><span class="err"> </span><span class="o">-</span><span class="err"> </span><span class="nb">len</span><span class="p">(</span><span class="n">mp_dist</span><span class="p">)</span>
<span class="err">    </span><span class="n">filler</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">np</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n_fill</span><span class="p">)</span>
<span class="err">    </span><span class="n">filler</span><span class="p">.</span><span class="n">fill</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">)</span>
<span class="err">    </span><span class="n">mp_dist</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">filler</span><span class="p">,</span><span class="err"> </span><span class="n">mp_dist</span><span class="p">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)))</span>
<span class="err">    </span>
<span class="err">    </span><span class="c1"># save the mp
</span><span class="err">    </span><span class="n">mp_dists</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">mp_dist</span>
</code></pre></div></div>

<p>So my, admittedly simplistic (i like to do the dumbest thing i can get to work), approach to ranking each matrix profile was to score them based on the distance between the 99.5th percentile (e.g. very largest values) and the 80th percentile. The idea here is that a good looking matrix profile is one with clear spikes where the highest matrix profile values as as far away as possible from all the other matrix profile values. In this setting, a higher score (distance between 99.5th and 80th percentile) should tend to be a more “interesting” matrix profile for the purpose of anomaly detection.</p>

<p>Below is a nice clear example of what i mean. The line on the top is the raw time series while the orange line below is the computed matrix profile of subsequence distances (my sequence length in this case was 60 for 1 minute). We can see that the matrix profile distances clearly jump and ‘stand out’ from all the other matrix profile values just as the ‘step’ occurs in the raw data. So this is a nice example where its worked pretty good on this somewhat simplistic example of a level shift type anomaly.</p>

<p><img src="/assets/images/2021-02-16-anomaly-detection-using-matrix-profile/image-1-1024x335.png" alt="" /></p>

<p>This approach generally does a good job at finding anomalies like that above. Sometimes it’s actually even maybe too fancy for me and picks out cases like the next one, where it seems to really focus on the appearance of one on/off step in between a more regular cycle of on/off traffic. This is actually the hardest part about anomaly detection, for sure it has found a novel or interesting unusual pattern below but it’s most likely not an anomaly in the sense that a user in my domain might care about.</p>

<p><img src="/assets/images/2021-02-16-anomaly-detection-using-matrix-profile/image-2-1024x331.png" alt="" /></p>

<p>Here is an example that might make this point a bit more obvious, it seems for certain types of messy jumpy and somewhat random data the matrix profile based anomaly detection might be a little too sensitive for what i need.</p>

<p><img src="/assets/images/2021-02-16-anomaly-detection-using-matrix-profile/image-3-1024x332.png" alt="" /></p>

<p>This gets us into the even more difficult aspect of anomaly detection - the subjective nature of it all, different types of time series and different domains you are working in - it all matters.</p>

<p>Here is probably one of the worst way’s this approach did not quite work for me. It seems to have really gotten excited about a particularly flat part of the time series and fully ignored the huge spike over to the right which is the more obvious (to a human eye) anomaly.</p>

<p><img src="/assets/images/2021-02-16-anomaly-detection-using-matrix-profile/image-4-1024x331.png" alt="" /></p>

<p>So its a case of needing to go back to the drawing board on this one after some initial prototyping. I think i might need to think a bit more about various preprocessing steps i might want to do like smoothing, differencing etc. and think about more about my scoring metric to maybe get something a little be more nuannced perhaps. Or more realisticly, but less sexy, additional post processing heuristics and rules to filter out “false positives” (in some sense) based on domain specific knowledge for my use case.</p>

<p>Anyway, just wanted to share this quick little example of finding something fancy, implementing a quick prototype, finding mixed results and then iterating on this cycle of dispair as is what 80% of DS/ML is like in reality :)</p>

<p>Onto the next fancy thing in the hope of glory!</p>

<p><strong>Update</strong>: Sean Law (<a href="https://twitter.com/seanmylaw">@seanmylaw</a>), one of the maintainers of STUMPY, <a href="https://twitter.com/seanmylaw/status/1368405672585424898">tweeted to me</a> about trying out `normalize=False` parameter as it might help with my use case. Pleased to say that it seems to have helped a lot from some initial results so i am going to continue to explore this as another potentially different but complementary approach to some anomaly detection work i am doing.</p>


<style>
.cover-image {
  margin-bottom: 2em;
  max-width: 100%;
  overflow: hidden;
}

.cover-image img {
  width: 100%;
  height: auto;
  display: block;
}

.post-meta {
  color: #666;
  margin-bottom: 2em;
}

.category {
  font-style: italic;
}

figure {
  margin: 1.5em 0;
  text-align: center;
}

figure img {
  max-width: 100%;
  height: auto;
  display: block;
  margin: 0 auto;
}

figcaption {
  color: #666;
  font-style: italic;
  margin-top: 0.5em;
}

img {
  max-width: 100%;
  height: auto;
  display: block;
  margin: 1em auto;
}

code {
  padding: 0.2em 0.4em;
  background-color: #f6f8fa;
  border-radius: 3px;
  font-size: 85%;
}

pre code {
  padding: 0;
  background-color: transparent;
}

.language-plaintext {
  color: #24292e;
}

.language-python {
  color: #24292e;
}
</style> 

      </section>
      <footer>
        
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
  </body>
</html>
