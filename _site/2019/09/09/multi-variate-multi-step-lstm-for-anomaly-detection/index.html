<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Multi-Variate, Multi-Step, LSTM for Anomaly Detection | Andrew M Blog</title>
<meta name="generator" content="Jekyll v3.9.3" />
<meta property="og:title" content="Multi-Variate, Multi-Step, LSTM for Anomaly Detection" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This post will walk through a synthetic example illustrating one way to use a multi-variate, multi-step LSTM for anomaly detection." />
<meta property="og:description" content="This post will walk through a synthetic example illustrating one way to use a multi-variate, multi-step LSTM for anomaly detection." />
<link rel="canonical" href="http://localhost:4000/2019/09/09/multi-variate-multi-step-lstm-for-anomaly-detection/" />
<meta property="og:url" content="http://localhost:4000/2019/09/09/multi-variate-multi-step-lstm-for-anomaly-detection/" />
<meta property="og:site_name" content="Andrew M Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-09-09T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Multi-Variate, Multi-Step, LSTM for Anomaly Detection" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2019-09-09T00:00:00+01:00","datePublished":"2019-09-09T00:00:00+01:00","description":"This post will walk through a synthetic example illustrating one way to use a multi-variate, multi-step LSTM for anomaly detection.","headline":"Multi-Variate, Multi-Step, LSTM for Anomaly Detection","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2019/09/09/multi-variate-multi-step-lstm-for-anomaly-detection/"},"url":"http://localhost:4000/2019/09/09/multi-variate-multi-step-lstm-for-anomaly-detection/"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="http://localhost:4000/">Andrew M Blog</a></h1>

        

        <p>Personal blog about Data Science and Machine Learning, migrated from WordPress.
</p>

        

        

        
      </header>
      <section>

      

<h1>Multi-Variate, Multi-Step, LSTM for Anomaly Detection</h1>

<p class="post-meta">
  <time datetime="2019-09-09T00:00:00+01:00">Sep 9, 2019
  </time><span class="categories">
    ‚Ä¢ Categories: 
    
    <span class="category">machine-learning</span>
    
    
  </span></p>



<p>This post will walk through a synthetic example illustrating one way to use a multi-variate, multi-step <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTM</a> for anomaly detection.</p>

<p>Imagine you have a matrix of¬†<strong>k time series</strong>¬†data coming at you at regular intervals and you look at the last¬†<strong>n observations</strong>¬†for each metric.</p>

<figure>

<img src="/assets/images/2019-09-09-multi-variate-multi-step-lstm-for-anomaly-detection/1_KJE0RC7793B8qPLWadDqVg.png

<figcaption>

A matrix of 5 metrics from period t to t-n

</figcaption>

</figure>

<p>One approach to doing anomaly detection in such a setting is to build a model to predict each metric over each time step in your forecast horizon and when you notice your prediction errors start to change significantly this can be a sign of some anomalies in your incoming data.</p>

<p>This is essentially an unsupervised problem that can be converted into a supervised one. You train the model to predict its own training data. Then once it gets good at this (assuming your training data is relatively typical of normal behavior of your data), if you see some new data for which your prediction error is much higher then expected, that can be a sign that you new data is anomalous in some way.</p>

<p><em>Note: This example is adapted and built off of</em>¬†<a href="https://developer.ibm.com/tutorials/iot-deep-learning-anomaly-detection-5/"><em>this tutorial</em></a>¬†<em>which i found a very useful starting point. All the code for this post is in</em>¬†<a href="https://github.com/andrewm4894/keras_learn/blob/master/lstm_multi.ipynb"><em>this notebook</em></a><em>. The rest of this post will essentially walk though the code.</em></p>

<h2 id="imports--paramaters">Imports &amp; Paramaters</h2>

<p>Below shows the imports and all the parameters for this example, you should be able to play with them and see what different results you get.</p>

<p><em>Note: There is a</em>¬†<a href="https://github.com/andrewm4894/keras_learn/blob/master/Pipfile"><em>Pipfile here</em></a>¬†<em>that shows the Python libraries needed. If you are not familiar, you should really check out</em>¬†<a href="https://pipenv.readthedocs.io/en/latest/basics/https://pipenv.readthedocs.io/en/latest/basics/"><em>pipenv</em></a><em>, its really useful once you play with it a bit.</em></p>

<p>https://gist.github.com/andrewm4894/4540e23d86fa02859191a38998661849#file-keras_lstm_multi_imports_and_params-py</p>

<h2 id="fake-data">Fake Data!</h2>

<p>We will generate some random data, and then smooth it out to look realistic. This will be our¬†<strong>‚Äònormal‚Äô data</strong>¬†that we will use to train the model.</p>

<figure>

<img src="/assets/images/2019-09-09-multi-variate-multi-step-lstm-for-anomaly-detection/1_HyFQhws3uVNRxYN6jBIzCw.jpeg

<figcaption>

I couldn't help myself.

</figcaption>

</figure>

<p>Then we will make a copy of this normal data and inject in some random noise at a certain point and for a period of time. This will be our¬†<strong>‚Äòbroken‚Äô data</strong>.</p>

<p>So this ‚Äòbroken‚Äô data is the data that we should see the model struggle with in terms of prediction error. It‚Äôs this error (aggregated and summarized in some way, e.g. turned into a z-score) that you could then use to drive an anomaly score (you could also use loss from the continually re-training on new data whereby the training loss should initially spike once the broken data comes into the system but over time the training would then adapt the model to the new data).</p>

<p>https://gist.github.com/andrewm4894/417f54744ccf1e8a909c9e373abd21fd#file-keras_lstm_multi_generate_fake_data_normal-py</p>

<p>This gives us our normal-ish real word looking data that we will use to train the model.</p>

<figure>

<img src="/assets/images/2019-09-09-multi-variate-multi-step-lstm-for-anomaly-detection/1_pJdOGJ3WMJy1qCcqbr6Rww.jpeg

<figcaption>

5 random time series that have been smoothed a bit to look realistic.

</figcaption>

</figure>

<p>To make our ‚Äòbroken‚Äô data (called data_new in the code) i lazily just copy the ‚Äònormal‚Äô data but mess up a segment of it with some random noise.</p>

<p>https://gist.github.com/andrewm4894/62174bbf299ecb36cada254314290644#file-keras_lstm_multi_generate_fake_data_broken-py</p>

<p>And so below we can see our ‚Äòbroken‚Äô data. I‚Äôve set the broken segment to be quite wide here and its very obvious the broken data is totally different. The hope is that in reality the model once trained would be good at picking up much more nuanced changes in the data that are less obvious to the human eye.</p>

<p>For example if all metrics were to suddenly become more or less correlated than normal but all still each move by a typical amount individually then this is the sort of change you‚Äôd like the model to highlight (this is probably something i should have tried to do when making the ‚Äòbroken‚Äô data to make the whole example more realistic, feel free to try this yourself and let me know how you get on).</p>

<figure>

<img src="/assets/images/2019-09-09-multi-variate-multi-step-lstm-for-anomaly-detection/1_UKqQRjsqMTPrRcpB7uEQbg.jpeg

<figcaption>

Same as the ‚Äúnormal‚Äù data but i‚Äôve messed up a huge chunk of it.

</figcaption>

</figure>

<h2 id="some-helper-functions">Some Helper Functions</h2>

<p>I‚Äôve built some helper functions to make life easier in the example notebook. I‚Äôll share the code below and talk a little about each.</p>

<ul>
  <li><strong>data_reshape_for_model()</strong>¬†: This function basically takes in an typical dataframe type array, loops through that data and reshapes it all into a numpy array of the shape expected by the keras LSTM model for both training and prediction. Figuring out how to reshape the data based on the N_TIMESTEPS, N_FEATURES and length of the data was actually probably the trickiest part of this whole example. I‚Äôve noticed that many tutorials online just reshape the data but do so in an incomplete way by essentially just pairing off rows. But what you really want to do is step through all the rows to make sure you roll your N_TIMESTEPS window properly over the data to as to all possible windows in your training.</li>
  <li><strong>train()</strong>¬†: This is just a simple wrapper for the keras train function. There is no real need for it.</li>
  <li><strong>predict()</strong>¬†: Similar to train() is just a wrapper function that does not really do much.</li>
  <li><strong>model_data_to_df_long()</strong>¬†: This function takes in a data array as used by the keras model and unrolls it into one big long pandas dataframe (numpy arrays freak me out a bit sometimes so i always try fall back pandas when i can get away with it üòâ).</li>
  <li><strong>model_df_long_to_wide()</strong>¬†: This function then takes the long format dataframe created by model_data_to_df_long() and converts it into a wide format that is closed to the original dataset of one row one observation and one column for each input feature (plus lots more columns for predictions for each feature for each timestep).</li>
  <li><strong>df_out_add_errors()</strong>¬†: This function adds errors and error aggregation columns to the main df_out dataframe which stores all the predictions and errors for each original row of data.</li>
  <li><strong>yhat_to_df_out()</strong>¬†: This function take‚Äôs in the model formatted training data and model formatted prediction outputs and wraps all the above functions to make a nice little ‚Äúdf_out‚Äù dataframe that has everything we want in it and is one row one observation so lines up more naturally with the original data.</li>
</ul>

<p>https://gist.github.com/andrewm4894/58179c6e240163a45dc2acff1843f7f2#file-keras_lstm_multi_helper_functions-py</p>

<h2 id="build--train-the-model">Build &amp; Train The Model</h2>

<p>Below code builds the model, trains it and also calls predict on all the training data be able to get errors on the original ‚Äònormal‚Äô training data.</p>

<p>https://gist.github.com/andrewm4894/48fa72cf730eead43f3ce54cdb3842cb#file-keras_lstm_multi_build_and_train-py</p>

<p>We then call our ‚Äúdo everything‚Äù yhat_to_df_out() function on the training data and the predictions from the model.</p>

<p>https://gist.github.com/andrewm4894/9b8c108661fc640325f6be774a0a1861#file-keras_lstm_multi_make_df_out-py</p>

<p>Now we can plot lots of things from df_out. For example here are the errors averaged across all five features are each timestep prediction horizon.</p>

<p>https://gist.github.com/andrewm4894/50bf96cc87ac37079c55516f4068a4b6#file-keras_lstm_multi_plot_error_avg-py</p>

<p><img src="/assets/images/2019-09-09-multi-variate-multi-step-lstm-for-anomaly-detection/1_6HBimTV7Bete2b2_iB9gTw.jpeg" alt="" /></p>

<p>In the above plot we can see the averaged error of the model on its training data. Each line represents a different forecast horizon. We can see that the lines are sort of ‚Äòstacked‚Äô on top of each other which makes sense as you‚Äôd generally expect the error 5 timesteps out (red line ‚Äút4_error_avg‚Äù) to be higher then the one step ahead forecast (greeny/orangy line ‚Äút0_error_avg‚Äù).</p>

<p>If we look at the standard deviation of our errors in a similar way, we can see how the standard deviation of our errors generally tends to increase at times when our 5 original features are diverging from each other as you can imagine these are the hardest parts of our time series for this model to predict.</p>

<p><img src="/assets/images/2019-09-09-multi-variate-multi-step-lstm-for-anomaly-detection/1_PuMxLqfI3xQkoTzNo44v1g.jpeg" alt="" /></p>

<h2 id="lets-break-it">Lets Break It</h2>

<p>So now that we have our model trained on our ‚Äònormal‚Äô data we can use it to see how well it does at predicting our new ‚Äòbroken‚Äô data.</p>

<p>https://gist.github.com/andrewm4894/fdb8ac2b8e8624e2a13e84b399f0faa1#file-keras_lstm_multi_build_and_train_new_broken_data-py</p>

<figure>

<img src="/assets/images/2019-09-09-multi-variate-multi-step-lstm-for-anomaly-detection/1_qB4T52kMb6VdRHsKT4gXbw.jpeg

<figcaption>

Here we can see that as soon as we hit the broken data the prediction errors go through the roof.

</figcaption>

</figure>

<p>From the above we can see that as soon as the random broken data comes into the time series the model prediction errors explode.</p>

<p>As mentioned, this is a very obvious and synthetic use case just for learning on but the main idea is that if your data changed in a more complicated and harder to spot way then your error rates would everywhere reflect this change. These error rates could then be used as input into a more global anomaly score for your system.</p>

<p>That‚Äôs it, thanks for reading and feel free to add any comments or questions below. I may add some more complicated or real world examples building on this approach at a later stage.</p>

<p><strong>UPDATE</strong>:¬†<a href="https://colab.research.google.com/drive/1Mx2uoeGL3VRQLifNtrcPY0rABc1GY3ep">Here</a>¬†is a Google Colab notebook that‚Äôs a bit better as i‚Äôve worked a bit more on this since the original blog post.</p>


<style>
.cover-image {
  margin-bottom: 2em;
  max-width: 100%;
  overflow: hidden;
}

.cover-image img {
  width: 100%;
  height: auto;
  display: block;
}

.post-meta {
  color: #666;
  margin-bottom: 2em;
}

.category {
  font-style: italic;
}

figure {
  margin: 1.5em 0;
  text-align: center;
}

figure img {
  max-width: 100%;
  height: auto;
  display: block;
  margin: 0 auto;
}

figcaption {
  color: #666;
  font-style: italic;
  margin-top: 0.5em;
}

img {
  max-width: 100%;
  height: auto;
  display: block;
  margin: 1em auto;
}

code {
  padding: 0.2em 0.4em;
  background-color: #f6f8fa;
  border-radius: 3px;
  font-size: 85%;
}

pre code {
  padding: 0;
  background-color: transparent;
}

.language-plaintext {
  color: #24292e;
}

.language-python {
  color: #24292e;
}
</style> 

      </section>
      <footer>
        
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
  </body>
</html>
