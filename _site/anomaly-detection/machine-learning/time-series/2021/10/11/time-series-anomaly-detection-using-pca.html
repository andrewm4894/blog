<figure>

![](/assets/images/2021-10-11-time-series-anomaly-detection-using-pca/image-4.png)

<figcaption>

contaminated raw data vs anomaly score

</figcaption>

</figure>

<p>Here is a little recipe for using good old <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a> to do some fast and efficient time series anomaly detection.</p>

<p>The high level idea here is to:</p>

<ol>
  <li>“featurize” the time series data into a traditional feature vector based formulation over recent data.</li>
  <li>fit a PCA model on some “mostly” normal data.</li>
  <li>when new data arrives if the PCA model is not able to do a good enough job at representing that data then that could be a sign of some anomalous observations.</li>
  <li>use some min/max normalisation tricks on the training data to get a nice “anomaly score” on a 0-100 scale.</li>
</ol>

<p><strong>Note</strong>: You can run and play with <a href="https://github.com/andrewm4894/colabs/blob/master/time_series_anomaly_detection_with_pca.ipynb">this colab notebook</a> if you’d rather just look at the code (<a href="https://github.com/andrewm4894/colabs/blob/master/time_series_anomaly_detection_with_pca.ipynb">github</a>, <a href="https://colab.research.google.com/drive/1kp7a-FANlwHrAwT2F7IIt7Af-gnDYJ3b?usp=sharing">colab</a>).</p>

<h2 id="raw-data">Raw Data</h2>

<p>Let’s begin by getting some raw data to work with. Here i will use public data from a demo server at <a href="https://www.netdata.cloud/">Netdata Cloud</a> where i work. You can see this data on the <a href="http://london.my-netdata.io/">Netdata dashboard</a> here or via its api <a href="https://london.my-netdata.io/api/v1/data?chart=system.cpu&amp;after=-3600&amp;before=0">here</a> (which is what we will be using - albeit via our <a href="https://github.com/netdata/netdata-pandas">netdata_pandas</a> package that manages all that).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># inputs 
</span><span class="n">host</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="s">'london.my-netdata.io'</span><span class="err">  </span><span class="c1"># pull from 'london' netdata demo host
</span><span class="n">after</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="o">-</span><span class="mi">3600</span><span class="err">  </span><span class="c1"># last 60 minutes
</span><span class="n">before</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="mi">0</span><span class="err">  </span><span class="c1"># starting from now
</span><span class="n">dims</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="p">[</span><span class="s">'system.cpu|system'</span><span class="p">]</span><span class="err">  </span><span class="c1"># lets just look at syatem cpu data
</span>
<span class="c1"># params
</span><span class="n">n_train</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="mi">3000</span><span class="err">  </span><span class="c1"># use the last 50 minutes of data to train on
</span><span class="n">diffs_n</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="mi">1</span><span class="err">  </span><span class="c1"># take differences
</span><span class="n">lags_n</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="mi">3</span><span class="err">  </span><span class="c1"># include 3 lags in the feature vector
</span><span class="n">smooth_n</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="mi">3</span><span class="err">  </span><span class="c1"># smooth the latest values to be included in the feature vector
</span>
<span class="c1"># get raw data
</span><span class="n">df</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">get_data</span><span class="p">(</span>
<span class="err">    </span><span class="n">hosts</span><span class="o">=</span><span class="p">[</span><span class="n">host</span><span class="p">],</span><span class="err"> </span>
<span class="err">    </span><span class="n">charts</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">([</span><span class="n">d</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">'|'</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="err"> </span><span class="k">for</span><span class="err"> </span><span class="n">d</span><span class="err"> </span><span class="ow">in</span><span class="err"> </span><span class="n">dims</span><span class="p">])),</span><span class="err"> </span>
<span class="err">    </span><span class="n">after</span><span class="o">=</span><span class="n">after</span><span class="p">,</span><span class="err"> </span>
<span class="err">    </span><span class="n">before</span><span class="o">=</span><span class="n">before</span><span class="p">,</span><span class="err"> </span>
<span class="err">    </span><span class="n">index_as_datetime</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
<span class="n">df</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">df</span><span class="p">[</span><span class="n">dims</span><span class="p">]</span>

<span class="c1"># look at raw data
</span><span class="k">print</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">head</span><span class="p">())</span>
</code></pre></div></div>

<p>This gives us our raw data from the last 60 minutes that we will be using.</p>

<figure>

![](/assets/images/2021-10-11-time-series-anomaly-detection-using-pca/image.png)

<figcaption>

raw data

</figcaption>

</figure>

<h2 id="add-some-anomalies">Add some anomalies</h2>

<p>To make things easy on ourselves we will take a little snippet of time towards the end of our raw data and mess it up by first shuffling it and then smoothing it so that it’s clearly anomalous.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create anomalous data
</span><span class="n">anomalous_len</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="nb">int</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="err"> </span><span class="o">-</span><span class="err"> </span><span class="n">n_train</span><span class="p">)</span><span class="err"> </span><span class="o">/</span><span class="err"> </span><span class="mi">2</span><span class="p">)</span><span class="err">  </span><span class="c1"># we pick half of our anomalous window to mess up
</span><span class="n">df_anomalous</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">df</span><span class="p">.</span><span class="n">tail</span><span class="p">(</span><span class="n">anomalous_len</span><span class="err"> </span><span class="o">+</span><span class="err"> </span><span class="n">anomalous_len</span><span class="p">)</span><span class="err">  </span><span class="c1"># get the tail end of our raw data
</span><span class="n">df_anomalous</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">df_anomalous</span><span class="p">.</span><span class="n">head</span><span class="p">(</span><span class="n">anomalous_len</span><span class="p">)</span><span class="err">  </span><span class="c1"># take the top part of it we want to mess with
</span><span class="n">df_anomalous</span><span class="p">[</span><span class="n">dims</span><span class="p">]</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">df_anomalous</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">values</span><span class="err">  </span><span class="c1"># scramble the data
</span><span class="n">df_anomalous</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">df_anomalous</span><span class="p">.</span><span class="n">rolling</span><span class="p">(</span><span class="mi">60</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span><span class="o">*</span><span class="mi">2</span><span class="err">  </span><span class="c1"># apply a 60 seconds rolling avg to smooth it so that it looks much different
</span>
<span class="c1"># append train data and anomalous data as 'contaminated' data
</span><span class="n">df_contaminated</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">df_train</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">df_anomalous</span><span class="p">).</span><span class="n">append</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">tail</span><span class="p">(</span><span class="n">anomalous_len</span><span class="p">)).</span><span class="n">interpolate</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s">'linear'</span><span class="p">)</span>
<span class="n">df_contaminated</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s">'contaminated data'</span><span class="p">,</span><span class="err"> </span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span><span class="err"> </span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Now we have data that look’s like below.</p>

<figure>

![](/assets/images/2021-10-11-time-series-anomaly-detection-using-pca/image-1.png)

<figcaption>

data with anomalous period added it

</figcaption>

</figure>

<h2 id="preprocessing">Preprocessing</h2>

<p>I have not mentioned it much but a key part of this “recipe” is in how we preprocess or featurize our raw time series data. We are keeping it simple here by ‘fattening’ our time series data at each timestep into a <em>differenced</em>, <em>smoothed</em> and <em>lagged</em> feature vector (which we also take absolute values of). This is typical basic feature preprocessing you might to when you want to convert your time series data into a more traditional ML formulation of observations of X’s and y’s.</p>

<p>I won’t talk about this much but <a href="https://machinelearningmastery.com/convert-time-series-supervised-learning-problem-python/">here</a> is a great blog post (all his stuff is great) that covers things in more detail.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">def</span><span class="err"> </span><span class="n">preprocess_df</span><span class="p">(</span><span class="n">df</span><span class="p">,</span><span class="err"> </span><span class="n">lags_n</span><span class="p">,</span><span class="err"> </span><span class="n">diffs_n</span><span class="p">,</span><span class="err"> </span><span class="n">smooth_n</span><span class="p">,</span><span class="err"> </span><span class="n">diffs_abs</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span><span class="err"> </span><span class="n">abs_features</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
<span class="err">    </span><span class="s">"""Given a pandas dataframe preprocess it to take differences, add smoothing, and lags as specified. 
    """</span>
<span class="err">    </span><span class="k">if</span><span class="err"> </span><span class="n">diffs_n</span><span class="err"> </span><span class="o">&gt;=</span><span class="err"> </span><span class="mi">1</span><span class="p">:</span>
<span class="err">        </span><span class="c1"># take differences
</span><span class="err">        </span><span class="n">df</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">df</span><span class="p">.</span><span class="n">diff</span><span class="p">(</span><span class="n">diffs_n</span><span class="p">).</span><span class="n">dropna</span><span class="p">()</span>
<span class="err">        </span><span class="c1"># abs diffs if defined
</span><span class="err">        </span><span class="k">if</span><span class="err"> </span><span class="n">diffs_abs</span><span class="err"> </span><span class="o">==</span><span class="err"> </span><span class="bp">True</span><span class="p">:</span>
<span class="err">            </span><span class="n">df</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="nb">abs</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="err">    </span><span class="k">if</span><span class="err"> </span><span class="n">smooth_n</span><span class="err"> </span><span class="o">&gt;=</span><span class="err"> </span><span class="mi">2</span><span class="p">:</span>
<span class="err">        </span><span class="c1"># apply a rolling average to smooth out the data a bit
</span><span class="err">        </span><span class="n">df</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">df</span><span class="p">.</span><span class="n">rolling</span><span class="p">(</span><span class="n">smooth_n</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">dropna</span><span class="p">()</span>
<span class="err">    </span><span class="k">if</span><span class="err"> </span><span class="n">lags_n</span><span class="err"> </span><span class="o">&gt;=</span><span class="err"> </span><span class="mi">1</span><span class="p">:</span>
<span class="err">        </span><span class="c1"># for each dimension add a new columns for each of lags_n lags of the differenced and smoothed values for that dimension
</span><span class="err">        </span><span class="n">df_columns_new</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="p">[</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">col</span><span class="si">}</span><span class="s">_lag</span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s">'</span><span class="err"> </span><span class="k">for</span><span class="err"> </span><span class="n">n</span><span class="err"> </span><span class="ow">in</span><span class="err"> </span><span class="nb">range</span><span class="p">(</span><span class="n">lags_n</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="err"> </span><span class="k">for</span><span class="err"> </span><span class="n">col</span><span class="err"> </span><span class="ow">in</span><span class="err"> </span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">]</span>
<span class="err">        </span><span class="n">df</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">.</span><span class="n">shift</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="err"> </span><span class="k">for</span><span class="err"> </span><span class="n">n</span><span class="err"> </span><span class="ow">in</span><span class="err"> </span><span class="nb">range</span><span class="p">(</span><span class="n">lags_n</span><span class="err"> </span><span class="o">+</span><span class="err"> </span><span class="mi">1</span><span class="p">)],</span><span class="err"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">dropna</span><span class="p">()</span>
<span class="err">        </span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">df_columns_new</span>
<span class="err">    </span><span class="c1"># sort columns to have lagged values next to each other for clarity when looking at the feature vectors
</span><span class="err">    </span><span class="n">df</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">df</span><span class="p">.</span><span class="n">reindex</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">columns</span><span class="p">),</span><span class="err"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="err">    </span><span class="c1"># abs all features if specified
</span><span class="err">    </span><span class="k">if</span><span class="err"> </span><span class="n">abs_features</span><span class="err"> </span><span class="o">==</span><span class="err"> </span><span class="bp">True</span><span class="p">:</span>
<span class="err">        </span><span class="n">df</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="nb">abs</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="err">    </span>
<span class="err">    </span><span class="k">return</span><span class="err"> </span><span class="n">df</span>

<span class="c1"># preprocess or 'featurize' the training data
</span><span class="n">train_data</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">preprocess_df</span><span class="p">(</span><span class="n">df_train</span><span class="p">,</span><span class="err"> </span><span class="n">lags_n</span><span class="p">,</span><span class="err"> </span><span class="n">diffs_n</span><span class="p">,</span><span class="err"> </span><span class="n">smooth_n</span><span class="p">)</span>

<span class="c1"># preprocess or 'featurize' the anomalous data
</span><span class="n">anomalous_data</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">preprocess_df</span><span class="p">(</span><span class="n">df_anomalous</span><span class="p">,</span><span class="err"> </span><span class="n">lags_n</span><span class="p">,</span><span class="err"> </span><span class="n">diffs_n</span><span class="p">,</span><span class="err"> </span><span class="n">smooth_n</span><span class="p">)</span>

<span class="c1"># preprocess or 'featurize' the contaminated data
</span><span class="n">contaminated_data</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">preprocess_df</span><span class="p">(</span><span class="n">df_contaminated</span><span class="p">,</span><span class="err"> </span><span class="n">lags_n</span><span class="p">,</span><span class="err"> </span><span class="n">diffs_n</span><span class="p">,</span><span class="err"> </span><span class="n">smooth_n</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="pca-ftw">PCA FTW!</h2>

<p>Now lets fit our PCA model on our training data and then use it to derive some anomaly scores for our anomalous data.</p>

<p>First we need to define exactly how we are going to use the PCA model to derive our anomaly scores.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">def</span><span class="err"> </span><span class="n">anomaly_scores</span><span class="p">(</span><span class="n">pca</span><span class="p">,</span><span class="err"> </span><span class="n">X</span><span class="p">):</span>
<span class="err">    </span><span class="s">"""Given a fitted pca model and some X feature vectors, compute an anomaly score as the sum of weighted euclidean distance between each sample to the
    hyperplane constructed by the selected eigenvectors. 
    """</span>
<span class="err">    </span><span class="k">return</span><span class="err"> </span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">cdist</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="err"> </span><span class="n">pca</span><span class="p">.</span><span class="n">components_</span><span class="p">)</span><span class="err"> </span><span class="o">/</span><span class="err"> </span><span class="n">pca</span><span class="p">.</span><span class="n">explained_variance_ratio_</span><span class="p">,</span><span class="err"> </span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">ravel</span><span class="p">()</span>
</code></pre></div></div>

<p>The function above takes in a fitted PCA and X vector of feature vectors and then computes a weighted distance measure between the features and the principle components. In a hand wavy sense the idea here is to measure the reconstruction error between the features X and our learned lower level representation of them that is implied via the fitted PCA mode (more details and references can be found in the awesome <a href="https://pyod.readthedocs.io/en/latest/_modules/pyod/models/pca.html">PyOD docs</a> - p.s. if you can you should just use PyOD as it has lots of different models implemented).</p>

<p>Now all we have to do is train or model, score our contaminated data and then apply our last little trick of min/max normalizing the raw anomaly scores based on the scores observed within the training data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># build PCA model
</span><span class="n">pca</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># scale based on training data
</span><span class="n">scaler</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>

<span class="c1"># fit model
</span><span class="n">pca</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">scaler</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_data</span><span class="p">))</span>

<span class="c1"># get anomaly scores for training data
</span><span class="n">train_scores</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">anomaly_scores</span><span class="p">(</span><span class="n">pca</span><span class="p">,</span><span class="err"> </span><span class="n">scaler</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_data</span><span class="p">))</span>
<span class="n">df_train_scores</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">train_scores</span><span class="p">,</span><span class="err"> </span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'anomaly_score'</span><span class="p">],</span><span class="err"> </span><span class="n">index</span><span class="o">=</span><span class="n">train_data</span><span class="p">.</span><span class="n">index</span><span class="p">)</span>
<span class="n">df_train_scores_min</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">df_train_scores</span><span class="p">.</span><span class="nb">min</span><span class="p">()</span>
<span class="n">df_train_scores_max</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">df_train_scores</span><span class="p">.</span><span class="nb">max</span><span class="p">()</span>

<span class="c1"># normalize anomaly scores on based training data
</span><span class="n">df_train_scores</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="p">(</span><span class="err"> </span><span class="n">df_train_scores</span><span class="err"> </span><span class="o">-</span><span class="err"> </span><span class="n">df_train_scores_min</span><span class="err"> </span><span class="p">)</span><span class="err"> </span><span class="o">/</span><span class="err"> </span><span class="p">(</span><span class="err"> </span><span class="n">df_train_scores_max</span><span class="err"> </span><span class="o">-</span><span class="err"> </span><span class="n">df_train_scores_min</span><span class="err"> </span><span class="p">)</span>

<span class="c1"># score all contaminated data
</span><span class="n">contaminated_scores</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">anomaly_scores</span><span class="p">(</span><span class="n">pca</span><span class="p">,</span><span class="err"> </span><span class="n">scaler</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">contaminated_data</span><span class="p">))</span>
<span class="n">df_contaminated_scores</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">contaminated_scores</span><span class="p">,</span><span class="err"> </span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'anomaly_score'</span><span class="p">],</span><span class="err"> </span><span class="n">index</span><span class="o">=</span><span class="n">contaminated_data</span><span class="p">.</span><span class="n">index</span><span class="p">)</span>

<span class="c1"># normalize based on train data scores
</span><span class="n">df_contaminated_scores</span><span class="err"> </span><span class="o">=</span><span class="err"> </span><span class="p">(</span><span class="err"> </span><span class="n">df_contaminated_scores</span><span class="err"> </span><span class="o">-</span><span class="err"> </span><span class="n">df_train_scores_min</span><span class="err"> </span><span class="p">)</span><span class="err"> </span><span class="o">/</span><span class="err"> </span><span class="p">(</span><span class="err"> </span><span class="n">df_train_scores_max</span><span class="err"> </span><span class="o">-</span><span class="err"> </span><span class="n">df_train_scores_min</span><span class="err"> </span><span class="p">)</span>
</code></pre></div></div>

<p>Now we can see we have our lovely normalized anomaly scores as below where we can see a clear elevation in the anomaly score during our anomalous window.</p>

<figure>

![](/assets/images/2021-10-11-time-series-anomaly-detection-using-pca/image-2.png)

<figcaption>

normalized anomaly scores

</figcaption>

</figure>

<p>As one last little bit of post processing we can apply a rolling average to our normalized anomaly scores to try and make our anomalous window stand out a little more.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_contaminated_scores_smoothed</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s">'contaminated data - smoothed anomaly score'</span><span class="p">,</span><span class="err"> </span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span><span class="err"> </span><span class="n">lw</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<figure>

![](/assets/images/2021-10-11-time-series-anomaly-detection-using-pca/image-3.png)

<figcaption>

smoothed normalized anomaly scores

</figcaption>

</figure>

<p>And there you have it, a fairly simple recipe with a bit of sensible feature engineering, good old reliable PCA, little normalization trick and you have a fairly robust, efficient and easy to implement anomaly detection playbook.</p>
